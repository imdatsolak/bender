[DEFAULT]
# Please set this to where Bender is located
root_dir = /home/iso/Development/bender

[bender-dev]
dev-server-ip = 10.0.2.152
dev-server-port = 5000
dev-server-access-log = %(root_dir)s/logs/bender-access.log


[bender-core]
name = bender
personality = q_a_machine
lowerconfidence_level = 0.60
higherconfidence_level = 0.9
# Reinforcement-timeout is seven (7) days
reinforcement_timeout = 604800
use_lookup = 0 
use_similarity = 1
core_languages = de
language = de_DE
supported_media_types = text
log_directory = %(root_dir)s/logs
log_hash = 0923ioisdjflksdfjoio23u490ueadaksjdxczhjdb78923409sdhkkf980324iu8sdfjkhkjsdf
bender_core_logfile = %(root_dir)s/logs/bender-core.log
interactive = 0
num_results = 1
use_hli = 1

[transientstorage]
module = modules.storage.transientstorage.mltransientstorage.MLTransientStorage

[permanentstorage]
module = modules.storage.permanentstorage.jsonstorage.JSONStorage
json_storage_database = %(root_dir)s/models/storage/generic-cache.json

[brain]
module = modules.brain.mlbrain.MLBrain
data_storage_path = %(root_dir)s/models/generic/brain 

[lookup]
module = modules.lookup.mllookup.MLLookup

[similarity]
module = modules.similarity.lsi.lsisimilarity.LSISimilarity
# NOTE:
# These similarity threshold are the equivalents of confidenceLevels in bender-core section above
# Since our GenSim is quite conservative, we need to scale the ranges down to find the best 
# possible answer. 
# SO: 
#   * similarity_higher_threshold - similarity_lower_threshold ~ higherconfidence_level - lowerconfidence_level
#   * 1 - similarity_higher_threshold ~ 1 - higherconfidence_level
# 
# You can use any similarity threshold between 0 and 1; the ranges will be automatically scaled when
# reporting back to BenderCore so that anything > similarity_higher_threshold is equivalent to a response
# from a machine with confLevel > higherconfidence_level
similarity_lower_threshold = 0.60
similarity_higher_threshold = 0.9

# this needs to be higher than "num_results" in the "bender-core" section
max_similarity_to_return = 20

# This is a master machine-logic, i.e., its results will be used to calculate final
# results...
is-master = 1

# if is-master == 1, contribution factor is not used, but should be set to any
# arbitrary number anyway
contribution-factor = 1000

[session]
module = modules.session.mlsession.MLSession
session_timeout = 10400

[nlp]
module = modules.nlp.spacynlp.SpacyNLP
language_model = en
language = en

[machinelogic-0]
name = samplenn
server_uri = http://localhost:9999/api/v1/query
accepted_languages = de_DE, en_US
accepted_media_types = text
returned_media_types = text
returns_response_id = 1
always_ask = 1
is-master = 0
contribution-factor = 500


[i-machinelogic-0]
name = annoy
module = modules.machinelogic.imachinelogic.annoy.annoysim.AnnoySim
training_module = modules.machinelogic.imachinelogic.annoy.annoysim.AnnoySimTrainer
annoy_data_path = %(root_dir)s/models/generic/annoy
accuracy = 2500
max_results = 50
# This is not a master machine, we use it to contribute to the 
# master machines results
is-master = 0
# Our contribution factor is 0.05 (=5%)
contribution-factor = 500

# NOTE: This is the retraining-check interval. If retraining is required, all machines will
# retrain. In order to not block the cpu by many machines training at the same time, please use
# different re-training intevals for the machines. The best way is actually to use
# prime-numbers. Therefore, here we use 5 minutes. In i-machinelogic-0 I use 7 minutes
# The minimum is five (5) minutes...
retraining_interval_in_minutes = 7


[i-machinelogic-1]
name = wmd
module = modules.machinelogic.imachinelogic.wmd.wmdlogicmt.WMDLogicMT
training_module = modules.machinelogic.imachinelogic.wmd.wmdlogicmt.WMDLogicMTTrainer
wmd_data_path = %(root_dir)s/models/generic/wmd
word2vec_model_filename = %(root_dir)s/models/generic/vectors/word2vec_orig.embeddings
max_wmd_results = 100
wmd_lower_threshold = 0.60
wmd_higher_threshold = 0.9
wmd_num_instances = 2
wmd_timeout = 30
# # This is not a master machine, we use it to contribute to the 
# # master machines results
is-master = 0
# # Our contribution factor is 0.05 (=5%)
contribution-factor = 500
retraining_interval_in_minutes = 5

[response-postprocessor]
module = modules.response.mlresponseproc.MLResponseProcessor

[indexed-response-processor]
module = modules.indexedresponse.indexedresponse.IndexedResponseProcessor

[humanlogic]
module = modules.humanlogic.mlhumanlogic.MLHumanLogic

[concept]
module = modules.concept.mlconcept.MLConcept

[stt]
module = modules.speech.mlstt.MLSpeechToText

[tts]
module = modules.speech.mltts.MLTextToSpeech

[dataextractor-0]
name = Person Name Extractor
module = modules.dataextractors.nameextractor.PersonDetector
language = de
female-names-file = %(root_dir)s/resources/names/fem-names.txt
male-names-file = %(root_dir)s/resources/names/male-names.txt
unisex-names-file = %(root_dir)s/resources/names/mixed-names-sorted.txt
last-names-file = %(root_dir)s/resources/names/last-names.txt
use-names = 0

[dataextractor-1]
name = Person Extractor
module = modules.dataextractors.nameextractor.UserNameExtractor

[dataprovider-0]
name = Sample Data Provider
module = modules.dataproviders.mldataprovider.MLDataProvider

[datainfusor]
module = modules.datainfusor.mldatainfusor.MLDataInfusor

[preprocessor-0]
module = modules.requestproc.mlreqproc.MLRequestProcessor
name = ML Request Preprocessor

# [spelling]
# MLSpelling is a dummy speller, returning always 'True' and if 
# asked to suggest, it returns the same word
# module = modules.spelling.mlspelling.MLSpelling

[spelling]
# HunSpelling requires "hunspelling"

module = modules.spelling.hunspell.hunspelling.HunSpelling
spelling-dict-file = %(root_dir)s/resources/dictionaries/de/de_DE.dic
spelling-aff-file = %(root_dir)s/resources/dictionaries/de/de_DE.aff
training-add-words-from-file = %(root_dir)s/data/sources/generic/spelling.txt

# Tokenizer language, one of:
# 	czech
# 	danish
# 	dutch
# 	english
# 	estonian
# 	finnish
# 	french
# 	german
# 	greek
# 	italian
# 	norwegian
# 	polish
# 	portuguese
# 	slovene
# 	spanish
# 	swedish
# 	turkish

tokenizer-language = english


# [spelling]
# EnchantSpelling requires "Enchant" & "PyEnchant"
# module = modules.spelling.enchant.enchantspelling.EnchantSpelling
# spelling-language-full = de
# training-add-words-from-file = %(root_dir)s/data/sources/generic/spelling.txt

# Tokenizer language, one of:
# 	czech
# 	danish
# 	dutch
# 	english
# 	estonian
# 	finnish
# 	french
# 	german
# 	greek
# 	italian
# 	norwegian
# 	polish
# 	portuguese
# 	slovene
# 	spanish
# 	swedish
# 	turkish

# tokenizer-language = german


[bender-training]
train_data_source_file = %(root_dir)s/test_data/generic/traindata.pickle
train_log_file = %(root_dir)s/logs/bender-train.log

# If you have a dictionary file (or files), you use the path to that directoy
# The dictionary file should contain *at least* ALL WORDS from ALL QUESTIONS and ALL ANSWERS
# You can have more words in the dictionary file (i.e. more than they appear in q's and a's),
# ... in fact: you should have a lot more words in your dictionary file, but the MINIMUM is ALL
# WORDS in ALL Q's and ALL A's
# Bender-train expects a list of .txt files in the directory below.
# You could actually get the EuroParl for your language and put the texts in this directory
# next you q/a file...
dictionary_data_source_path = %(root_dir)s/test_data/generic

output_path = %(root_dir)s/models/generic

# This for Bender internally. After training, DO NOT CHANGE THIS PATH
converted_train_data_q_path = %(output_path)s/queries
converted_train_data_a_path = %(output_path)s/answers

# Location where the dictionary should be stored...
# NOTE: This is a FOLDER! The actual file is called 'dictionary.dict' in this folder... It will be
# automatically managed by Bender-train
dictionary_output_path = %(output_path)s/dictionary

# Query media type is either single media type or a combination of text/<other-media-type>
# Examples:
# 		text/image : A question will be asked in 'text' about an 'image'
# 		text: A question will be asked (text-based comms)
# 		text/video : A question will be asked in 'text' about a 'video
query_media_type = text
# What is the media type in the response: can be one of text, image, audio, video, speech
response_media_type = text
train_data_q_media_type = text
train_data_a_media_type = text
# data_language_short: any of "N/A", "de", "en", ...
data_language_short = de
data_language_full = de_DE
remove_stop_words = 0
num_topics_lsi = 1000
generate_lsi = 1

# Number of dimensions for the embedding. Somewhere between 100...999
word2vec_dims = 500

# If a word occurs less than this many times in the source, it is clipped
word2vec_min_count = 1

# The distance between words in a sentence to understand the sentence meaning.
# English: 5 is enough
# German :  may need 7-10
# Other languages: your guess is as good as mine :-)
word2vec_window = 7

# Number of iterations during the generation of the embeddings
word2vec_iter = 20 

# The following is the number of worker-threads to use to generate embeddings
# NEVER set it higher than your CPU-Core-Count (or thread-count)
# ALWAYS have one or two cores/threads left for you
word2vec_workers = 12

# Should we save the original Word2Vec model for further use for other modules
# before converting it into a binary numpy-file?
# If so, where should we store it?
preserve_original_w2vmodel = 1
original_w2v_path = %(output_path)s/vectors

# This is the retraining interval
# After every this many minutes, Bender will check if it needs to re-train its internal machines
# and do so accordingly
# The minimum is 5 minutes, i.e., after at least every 5 minutes we will check if re-training
# is required
retraining_interval_in_minutes = 5

